{
  "best_global_step": 1689,
  "best_metric": 0.8657308220863342,
  "best_model_checkpoint": "c:\\Users\\yewon\\Coding\\GETIT\\BiasBuster_back\\app\\data\\models\\political_bias_model\\checkpoint-1689",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1689,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.017761989342806393,
      "grad_norm": 3.4562387466430664,
      "learning_rate": 4.9733570159857905e-05,
      "loss": 1.1742,
      "step": 10
    },
    {
      "epoch": 0.035523978685612786,
      "grad_norm": 5.389575004577637,
      "learning_rate": 4.943753700414447e-05,
      "loss": 1.0773,
      "step": 20
    },
    {
      "epoch": 0.05328596802841918,
      "grad_norm": 3.14986515045166,
      "learning_rate": 4.9141503848431024e-05,
      "loss": 1.0741,
      "step": 30
    },
    {
      "epoch": 0.07104795737122557,
      "grad_norm": 5.293546676635742,
      "learning_rate": 4.884547069271759e-05,
      "loss": 1.0688,
      "step": 40
    },
    {
      "epoch": 0.08880994671403197,
      "grad_norm": 5.842503070831299,
      "learning_rate": 4.8549437537004144e-05,
      "loss": 1.0584,
      "step": 50
    },
    {
      "epoch": 0.10657193605683836,
      "grad_norm": 5.856225490570068,
      "learning_rate": 4.825340438129071e-05,
      "loss": 1.0219,
      "step": 60
    },
    {
      "epoch": 0.12433392539964476,
      "grad_norm": 13.680242538452148,
      "learning_rate": 4.795737122557727e-05,
      "loss": 1.0512,
      "step": 70
    },
    {
      "epoch": 0.14209591474245115,
      "grad_norm": 4.897099018096924,
      "learning_rate": 4.766133806986383e-05,
      "loss": 1.2201,
      "step": 80
    },
    {
      "epoch": 0.15985790408525755,
      "grad_norm": 10.0615234375,
      "learning_rate": 4.736530491415039e-05,
      "loss": 1.0309,
      "step": 90
    },
    {
      "epoch": 0.17761989342806395,
      "grad_norm": 5.139016628265381,
      "learning_rate": 4.706927175843695e-05,
      "loss": 1.0926,
      "step": 100
    },
    {
      "epoch": 0.19538188277087035,
      "grad_norm": 3.256096601486206,
      "learning_rate": 4.6773238602723504e-05,
      "loss": 1.0061,
      "step": 110
    },
    {
      "epoch": 0.21314387211367672,
      "grad_norm": 3.1658856868743896,
      "learning_rate": 4.647720544701007e-05,
      "loss": 1.0779,
      "step": 120
    },
    {
      "epoch": 0.23090586145648312,
      "grad_norm": 4.004689693450928,
      "learning_rate": 4.6181172291296624e-05,
      "loss": 0.9728,
      "step": 130
    },
    {
      "epoch": 0.24866785079928952,
      "grad_norm": 2.717689275741577,
      "learning_rate": 4.588513913558319e-05,
      "loss": 1.1728,
      "step": 140
    },
    {
      "epoch": 0.2664298401420959,
      "grad_norm": 23.792789459228516,
      "learning_rate": 4.5589105979869744e-05,
      "loss": 1.0278,
      "step": 150
    },
    {
      "epoch": 0.2841918294849023,
      "grad_norm": 3.145634889602661,
      "learning_rate": 4.529307282415631e-05,
      "loss": 1.1275,
      "step": 160
    },
    {
      "epoch": 0.3019538188277087,
      "grad_norm": 3.314389944076538,
      "learning_rate": 4.499703966844287e-05,
      "loss": 1.0586,
      "step": 170
    },
    {
      "epoch": 0.3197158081705151,
      "grad_norm": 27.944414138793945,
      "learning_rate": 4.470100651272943e-05,
      "loss": 0.9408,
      "step": 180
    },
    {
      "epoch": 0.33747779751332146,
      "grad_norm": 29.797958374023438,
      "learning_rate": 4.440497335701599e-05,
      "loss": 1.0124,
      "step": 190
    },
    {
      "epoch": 0.3552397868561279,
      "grad_norm": 31.865318298339844,
      "learning_rate": 4.410894020130255e-05,
      "loss": 0.9931,
      "step": 200
    },
    {
      "epoch": 0.37300177619893427,
      "grad_norm": 25.9392032623291,
      "learning_rate": 4.381290704558911e-05,
      "loss": 1.0683,
      "step": 210
    },
    {
      "epoch": 0.3907637655417407,
      "grad_norm": 2.526986837387085,
      "learning_rate": 4.351687388987567e-05,
      "loss": 0.9873,
      "step": 220
    },
    {
      "epoch": 0.40852575488454707,
      "grad_norm": 4.615077495574951,
      "learning_rate": 4.322084073416223e-05,
      "loss": 1.0358,
      "step": 230
    },
    {
      "epoch": 0.42628774422735344,
      "grad_norm": 2.9556214809417725,
      "learning_rate": 4.2924807578448786e-05,
      "loss": 1.0721,
      "step": 240
    },
    {
      "epoch": 0.44404973357015987,
      "grad_norm": 6.491071701049805,
      "learning_rate": 4.262877442273534e-05,
      "loss": 1.0383,
      "step": 250
    },
    {
      "epoch": 0.46181172291296624,
      "grad_norm": 5.443248271942139,
      "learning_rate": 4.2332741267021906e-05,
      "loss": 1.0453,
      "step": 260
    },
    {
      "epoch": 0.47957371225577267,
      "grad_norm": 3.792642593383789,
      "learning_rate": 4.203670811130847e-05,
      "loss": 1.0324,
      "step": 270
    },
    {
      "epoch": 0.49733570159857904,
      "grad_norm": 10.248740196228027,
      "learning_rate": 4.1740674955595026e-05,
      "loss": 1.0658,
      "step": 280
    },
    {
      "epoch": 0.5150976909413855,
      "grad_norm": 3.0112125873565674,
      "learning_rate": 4.144464179988159e-05,
      "loss": 0.9853,
      "step": 290
    },
    {
      "epoch": 0.5328596802841918,
      "grad_norm": 6.274662494659424,
      "learning_rate": 4.1148608644168146e-05,
      "loss": 1.0329,
      "step": 300
    },
    {
      "epoch": 0.5506216696269982,
      "grad_norm": 14.337173461914062,
      "learning_rate": 4.085257548845471e-05,
      "loss": 1.0138,
      "step": 310
    },
    {
      "epoch": 0.5683836589698046,
      "grad_norm": 5.964857578277588,
      "learning_rate": 4.055654233274127e-05,
      "loss": 1.0079,
      "step": 320
    },
    {
      "epoch": 0.5861456483126111,
      "grad_norm": 2.0593974590301514,
      "learning_rate": 4.026050917702783e-05,
      "loss": 1.0075,
      "step": 330
    },
    {
      "epoch": 0.6039076376554174,
      "grad_norm": 10.277934074401855,
      "learning_rate": 3.996447602131439e-05,
      "loss": 1.0615,
      "step": 340
    },
    {
      "epoch": 0.6216696269982238,
      "grad_norm": 3.165070056915283,
      "learning_rate": 3.966844286560095e-05,
      "loss": 1.0678,
      "step": 350
    },
    {
      "epoch": 0.6394316163410302,
      "grad_norm": 21.13850975036621,
      "learning_rate": 3.9372409709887506e-05,
      "loss": 1.0931,
      "step": 360
    },
    {
      "epoch": 0.6571936056838366,
      "grad_norm": 6.912815093994141,
      "learning_rate": 3.907637655417407e-05,
      "loss": 1.0348,
      "step": 370
    },
    {
      "epoch": 0.6749555950266429,
      "grad_norm": 7.405005931854248,
      "learning_rate": 3.8780343398460625e-05,
      "loss": 0.9833,
      "step": 380
    },
    {
      "epoch": 0.6927175843694494,
      "grad_norm": 6.12532377243042,
      "learning_rate": 3.848431024274719e-05,
      "loss": 0.9965,
      "step": 390
    },
    {
      "epoch": 0.7104795737122558,
      "grad_norm": 4.0316386222839355,
      "learning_rate": 3.8188277087033745e-05,
      "loss": 0.9983,
      "step": 400
    },
    {
      "epoch": 0.7282415630550622,
      "grad_norm": 14.032527923583984,
      "learning_rate": 3.789224393132031e-05,
      "loss": 0.9303,
      "step": 410
    },
    {
      "epoch": 0.7460035523978685,
      "grad_norm": 21.404008865356445,
      "learning_rate": 3.759621077560687e-05,
      "loss": 0.9164,
      "step": 420
    },
    {
      "epoch": 0.7637655417406749,
      "grad_norm": 6.652446746826172,
      "learning_rate": 3.730017761989343e-05,
      "loss": 0.9499,
      "step": 430
    },
    {
      "epoch": 0.7815275310834814,
      "grad_norm": 3.1142776012420654,
      "learning_rate": 3.700414446417999e-05,
      "loss": 0.9614,
      "step": 440
    },
    {
      "epoch": 0.7992895204262878,
      "grad_norm": 18.665023803710938,
      "learning_rate": 3.670811130846655e-05,
      "loss": 0.9349,
      "step": 450
    },
    {
      "epoch": 0.8170515097690941,
      "grad_norm": 1.8348679542541504,
      "learning_rate": 3.641207815275311e-05,
      "loss": 1.0325,
      "step": 460
    },
    {
      "epoch": 0.8348134991119005,
      "grad_norm": 7.303642749786377,
      "learning_rate": 3.6116044997039675e-05,
      "loss": 0.9436,
      "step": 470
    },
    {
      "epoch": 0.8525754884547069,
      "grad_norm": 5.364692687988281,
      "learning_rate": 3.582001184132623e-05,
      "loss": 1.0297,
      "step": 480
    },
    {
      "epoch": 0.8703374777975134,
      "grad_norm": 11.965383529663086,
      "learning_rate": 3.552397868561279e-05,
      "loss": 0.9095,
      "step": 490
    },
    {
      "epoch": 0.8880994671403197,
      "grad_norm": 12.017311096191406,
      "learning_rate": 3.5227945529899345e-05,
      "loss": 1.0044,
      "step": 500
    },
    {
      "epoch": 0.9058614564831261,
      "grad_norm": 9.685199737548828,
      "learning_rate": 3.493191237418591e-05,
      "loss": 0.971,
      "step": 510
    },
    {
      "epoch": 0.9236234458259325,
      "grad_norm": 6.022009372711182,
      "learning_rate": 3.463587921847247e-05,
      "loss": 0.7981,
      "step": 520
    },
    {
      "epoch": 0.9413854351687388,
      "grad_norm": 19.788393020629883,
      "learning_rate": 3.433984606275903e-05,
      "loss": 0.8687,
      "step": 530
    },
    {
      "epoch": 0.9591474245115453,
      "grad_norm": 8.00395393371582,
      "learning_rate": 3.404381290704559e-05,
      "loss": 1.0407,
      "step": 540
    },
    {
      "epoch": 0.9769094138543517,
      "grad_norm": 33.504764556884766,
      "learning_rate": 3.374777975133215e-05,
      "loss": 1.1416,
      "step": 550
    },
    {
      "epoch": 0.9946714031971581,
      "grad_norm": 4.5941386222839355,
      "learning_rate": 3.345174659561871e-05,
      "loss": 0.958,
      "step": 560
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.516,
      "eval_f1": 0.44799175323298707,
      "eval_loss": 0.9234556555747986,
      "eval_precision": 0.40343459997729453,
      "eval_recall": 0.516,
      "eval_runtime": 164.0605,
      "eval_samples_per_second": 3.048,
      "eval_steps_per_second": 0.384,
      "step": 563
    },
    {
      "epoch": 1.0124333925399644,
      "grad_norm": 26.100543975830078,
      "learning_rate": 3.3155713439905274e-05,
      "loss": 0.9726,
      "step": 570
    },
    {
      "epoch": 1.030195381882771,
      "grad_norm": 3.2233474254608154,
      "learning_rate": 3.285968028419183e-05,
      "loss": 1.0869,
      "step": 580
    },
    {
      "epoch": 1.0479573712255772,
      "grad_norm": 12.925053596496582,
      "learning_rate": 3.2563647128478394e-05,
      "loss": 0.9484,
      "step": 590
    },
    {
      "epoch": 1.0657193605683837,
      "grad_norm": 2.0394935607910156,
      "learning_rate": 3.226761397276495e-05,
      "loss": 0.988,
      "step": 600
    },
    {
      "epoch": 1.0834813499111902,
      "grad_norm": 6.148420333862305,
      "learning_rate": 3.1971580817051514e-05,
      "loss": 1.0149,
      "step": 610
    },
    {
      "epoch": 1.1012433392539964,
      "grad_norm": 18.6698055267334,
      "learning_rate": 3.167554766133807e-05,
      "loss": 0.9828,
      "step": 620
    },
    {
      "epoch": 1.119005328596803,
      "grad_norm": 37.64730453491211,
      "learning_rate": 3.137951450562463e-05,
      "loss": 0.8579,
      "step": 630
    },
    {
      "epoch": 1.1367673179396092,
      "grad_norm": 19.43450164794922,
      "learning_rate": 3.108348134991119e-05,
      "loss": 1.0906,
      "step": 640
    },
    {
      "epoch": 1.1545293072824157,
      "grad_norm": 34.20695114135742,
      "learning_rate": 3.0787448194197754e-05,
      "loss": 0.9058,
      "step": 650
    },
    {
      "epoch": 1.1722912966252221,
      "grad_norm": 9.413626670837402,
      "learning_rate": 3.049141503848431e-05,
      "loss": 0.9474,
      "step": 660
    },
    {
      "epoch": 1.1900532859680284,
      "grad_norm": 2.290215015411377,
      "learning_rate": 3.0195381882770874e-05,
      "loss": 0.8952,
      "step": 670
    },
    {
      "epoch": 1.2078152753108349,
      "grad_norm": 26.62515640258789,
      "learning_rate": 2.989934872705743e-05,
      "loss": 0.8857,
      "step": 680
    },
    {
      "epoch": 1.2255772646536411,
      "grad_norm": 4.946177005767822,
      "learning_rate": 2.9603315571343993e-05,
      "loss": 1.0109,
      "step": 690
    },
    {
      "epoch": 1.2433392539964476,
      "grad_norm": 8.326519012451172,
      "learning_rate": 2.9307282415630553e-05,
      "loss": 1.0033,
      "step": 700
    },
    {
      "epoch": 1.261101243339254,
      "grad_norm": 3.4667134284973145,
      "learning_rate": 2.901124925991711e-05,
      "loss": 0.8353,
      "step": 710
    },
    {
      "epoch": 1.2788632326820604,
      "grad_norm": 9.129932403564453,
      "learning_rate": 2.8715216104203673e-05,
      "loss": 0.9766,
      "step": 720
    },
    {
      "epoch": 1.2966252220248669,
      "grad_norm": 9.981573104858398,
      "learning_rate": 2.841918294849023e-05,
      "loss": 0.9511,
      "step": 730
    },
    {
      "epoch": 1.3143872113676731,
      "grad_norm": 40.70829772949219,
      "learning_rate": 2.8123149792776793e-05,
      "loss": 0.9113,
      "step": 740
    },
    {
      "epoch": 1.3321492007104796,
      "grad_norm": 15.82121753692627,
      "learning_rate": 2.7827116637063356e-05,
      "loss": 0.9275,
      "step": 750
    },
    {
      "epoch": 1.349911190053286,
      "grad_norm": 6.685775279998779,
      "learning_rate": 2.7531083481349913e-05,
      "loss": 0.9646,
      "step": 760
    },
    {
      "epoch": 1.3676731793960923,
      "grad_norm": 6.94476842880249,
      "learning_rate": 2.7235050325636473e-05,
      "loss": 0.9519,
      "step": 770
    },
    {
      "epoch": 1.3854351687388988,
      "grad_norm": 34.80537414550781,
      "learning_rate": 2.693901716992303e-05,
      "loss": 0.9712,
      "step": 780
    },
    {
      "epoch": 1.403197158081705,
      "grad_norm": 22.13756561279297,
      "learning_rate": 2.6642984014209593e-05,
      "loss": 0.8512,
      "step": 790
    },
    {
      "epoch": 1.4209591474245116,
      "grad_norm": 8.613987922668457,
      "learning_rate": 2.6346950858496156e-05,
      "loss": 0.9754,
      "step": 800
    },
    {
      "epoch": 1.438721136767318,
      "grad_norm": 11.910297393798828,
      "learning_rate": 2.6050917702782713e-05,
      "loss": 0.882,
      "step": 810
    },
    {
      "epoch": 1.4564831261101243,
      "grad_norm": 15.526297569274902,
      "learning_rate": 2.5754884547069276e-05,
      "loss": 0.9015,
      "step": 820
    },
    {
      "epoch": 1.4742451154529308,
      "grad_norm": 46.105804443359375,
      "learning_rate": 2.5458851391355832e-05,
      "loss": 0.9273,
      "step": 830
    },
    {
      "epoch": 1.492007104795737,
      "grad_norm": 10.77200984954834,
      "learning_rate": 2.5162818235642392e-05,
      "loss": 1.0055,
      "step": 840
    },
    {
      "epoch": 1.5097690941385435,
      "grad_norm": 12.658778190612793,
      "learning_rate": 2.4866785079928952e-05,
      "loss": 0.9688,
      "step": 850
    },
    {
      "epoch": 1.52753108348135,
      "grad_norm": 12.632232666015625,
      "learning_rate": 2.4570751924215512e-05,
      "loss": 0.9788,
      "step": 860
    },
    {
      "epoch": 1.5452930728241563,
      "grad_norm": 55.60129928588867,
      "learning_rate": 2.4274718768502072e-05,
      "loss": 0.7723,
      "step": 870
    },
    {
      "epoch": 1.5630550621669625,
      "grad_norm": 12.047835350036621,
      "learning_rate": 2.3978685612788635e-05,
      "loss": 0.7727,
      "step": 880
    },
    {
      "epoch": 1.580817051509769,
      "grad_norm": 10.56425666809082,
      "learning_rate": 2.3682652457075195e-05,
      "loss": 0.9134,
      "step": 890
    },
    {
      "epoch": 1.5985790408525755,
      "grad_norm": 2.9383716583251953,
      "learning_rate": 2.3386619301361752e-05,
      "loss": 0.8164,
      "step": 900
    },
    {
      "epoch": 1.616341030195382,
      "grad_norm": 7.290555953979492,
      "learning_rate": 2.3090586145648312e-05,
      "loss": 0.9749,
      "step": 910
    },
    {
      "epoch": 1.6341030195381883,
      "grad_norm": 6.4431376457214355,
      "learning_rate": 2.2794552989934872e-05,
      "loss": 0.854,
      "step": 920
    },
    {
      "epoch": 1.6518650088809945,
      "grad_norm": 11.38941764831543,
      "learning_rate": 2.2498519834221435e-05,
      "loss": 0.9542,
      "step": 930
    },
    {
      "epoch": 1.669626998223801,
      "grad_norm": 27.307252883911133,
      "learning_rate": 2.2202486678507995e-05,
      "loss": 0.9685,
      "step": 940
    },
    {
      "epoch": 1.6873889875666075,
      "grad_norm": 13.832115173339844,
      "learning_rate": 2.1906453522794555e-05,
      "loss": 0.9697,
      "step": 950
    },
    {
      "epoch": 1.705150976909414,
      "grad_norm": 8.862215042114258,
      "learning_rate": 2.1610420367081115e-05,
      "loss": 0.9011,
      "step": 960
    },
    {
      "epoch": 1.7229129662522202,
      "grad_norm": 12.208176612854004,
      "learning_rate": 2.131438721136767e-05,
      "loss": 0.9631,
      "step": 970
    },
    {
      "epoch": 1.7406749555950265,
      "grad_norm": 30.16390609741211,
      "learning_rate": 2.1018354055654235e-05,
      "loss": 0.8982,
      "step": 980
    },
    {
      "epoch": 1.758436944937833,
      "grad_norm": 10.502854347229004,
      "learning_rate": 2.0722320899940795e-05,
      "loss": 0.9428,
      "step": 990
    },
    {
      "epoch": 1.7761989342806395,
      "grad_norm": 10.43616771697998,
      "learning_rate": 2.0426287744227355e-05,
      "loss": 0.9464,
      "step": 1000
    },
    {
      "epoch": 1.793960923623446,
      "grad_norm": 43.25046157836914,
      "learning_rate": 2.0130254588513915e-05,
      "loss": 0.7638,
      "step": 1010
    },
    {
      "epoch": 1.8117229129662522,
      "grad_norm": 14.695793151855469,
      "learning_rate": 1.9834221432800475e-05,
      "loss": 0.9466,
      "step": 1020
    },
    {
      "epoch": 1.8294849023090585,
      "grad_norm": 22.95578384399414,
      "learning_rate": 1.9538188277087034e-05,
      "loss": 1.0335,
      "step": 1030
    },
    {
      "epoch": 1.847246891651865,
      "grad_norm": 2.564889669418335,
      "learning_rate": 1.9242155121373594e-05,
      "loss": 0.8876,
      "step": 1040
    },
    {
      "epoch": 1.8650088809946714,
      "grad_norm": 19.518733978271484,
      "learning_rate": 1.8946121965660154e-05,
      "loss": 0.823,
      "step": 1050
    },
    {
      "epoch": 1.882770870337478,
      "grad_norm": 12.150784492492676,
      "learning_rate": 1.8650088809946714e-05,
      "loss": 0.8532,
      "step": 1060
    },
    {
      "epoch": 1.9005328596802842,
      "grad_norm": 18.753276824951172,
      "learning_rate": 1.8354055654233274e-05,
      "loss": 0.7476,
      "step": 1070
    },
    {
      "epoch": 1.9182948490230904,
      "grad_norm": 21.781423568725586,
      "learning_rate": 1.8058022498519837e-05,
      "loss": 0.9076,
      "step": 1080
    },
    {
      "epoch": 1.936056838365897,
      "grad_norm": 9.436890602111816,
      "learning_rate": 1.7761989342806394e-05,
      "loss": 0.9154,
      "step": 1090
    },
    {
      "epoch": 1.9538188277087034,
      "grad_norm": 12.956568717956543,
      "learning_rate": 1.7465956187092954e-05,
      "loss": 0.8277,
      "step": 1100
    },
    {
      "epoch": 1.97158081705151,
      "grad_norm": 34.751487731933594,
      "learning_rate": 1.7169923031379514e-05,
      "loss": 0.9112,
      "step": 1110
    },
    {
      "epoch": 1.9893428063943162,
      "grad_norm": 18.262025833129883,
      "learning_rate": 1.6873889875666074e-05,
      "loss": 0.7671,
      "step": 1120
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.574,
      "eval_f1": 0.5786131204022106,
      "eval_loss": 0.8705766797065735,
      "eval_precision": 0.5889515802116957,
      "eval_recall": 0.574,
      "eval_runtime": 160.7105,
      "eval_samples_per_second": 3.111,
      "eval_steps_per_second": 0.392,
      "step": 1126
    },
    {
      "epoch": 2.0071047957371224,
      "grad_norm": 17.350358963012695,
      "learning_rate": 1.6577856719952637e-05,
      "loss": 0.9614,
      "step": 1130
    },
    {
      "epoch": 2.024866785079929,
      "grad_norm": 12.185005187988281,
      "learning_rate": 1.6281823564239197e-05,
      "loss": 0.871,
      "step": 1140
    },
    {
      "epoch": 2.0426287744227354,
      "grad_norm": 15.260315895080566,
      "learning_rate": 1.5985790408525757e-05,
      "loss": 0.848,
      "step": 1150
    },
    {
      "epoch": 2.060390763765542,
      "grad_norm": 40.5975227355957,
      "learning_rate": 1.5689757252812314e-05,
      "loss": 0.7367,
      "step": 1160
    },
    {
      "epoch": 2.0781527531083483,
      "grad_norm": 84.8414535522461,
      "learning_rate": 1.5393724097098877e-05,
      "loss": 0.7873,
      "step": 1170
    },
    {
      "epoch": 2.0959147424511544,
      "grad_norm": 20.650529861450195,
      "learning_rate": 1.5097690941385437e-05,
      "loss": 0.7522,
      "step": 1180
    },
    {
      "epoch": 2.113676731793961,
      "grad_norm": 21.968950271606445,
      "learning_rate": 1.4801657785671997e-05,
      "loss": 0.9667,
      "step": 1190
    },
    {
      "epoch": 2.1314387211367674,
      "grad_norm": 8.421135902404785,
      "learning_rate": 1.4505624629958555e-05,
      "loss": 0.8368,
      "step": 1200
    },
    {
      "epoch": 2.149200710479574,
      "grad_norm": 41.94660949707031,
      "learning_rate": 1.4209591474245115e-05,
      "loss": 0.8059,
      "step": 1210
    },
    {
      "epoch": 2.1669626998223803,
      "grad_norm": 13.374176979064941,
      "learning_rate": 1.3913558318531678e-05,
      "loss": 0.9035,
      "step": 1220
    },
    {
      "epoch": 2.1847246891651864,
      "grad_norm": 9.807062149047852,
      "learning_rate": 1.3617525162818236e-05,
      "loss": 0.755,
      "step": 1230
    },
    {
      "epoch": 2.202486678507993,
      "grad_norm": 5.4199395179748535,
      "learning_rate": 1.3321492007104796e-05,
      "loss": 0.9985,
      "step": 1240
    },
    {
      "epoch": 2.2202486678507993,
      "grad_norm": 41.8331184387207,
      "learning_rate": 1.3025458851391356e-05,
      "loss": 0.7973,
      "step": 1250
    },
    {
      "epoch": 2.238010657193606,
      "grad_norm": 44.747718811035156,
      "learning_rate": 1.2729425695677916e-05,
      "loss": 0.8486,
      "step": 1260
    },
    {
      "epoch": 2.2557726465364123,
      "grad_norm": 2.8608527183532715,
      "learning_rate": 1.2433392539964476e-05,
      "loss": 0.7923,
      "step": 1270
    },
    {
      "epoch": 2.2735346358792183,
      "grad_norm": 36.554466247558594,
      "learning_rate": 1.2137359384251036e-05,
      "loss": 0.8941,
      "step": 1280
    },
    {
      "epoch": 2.291296625222025,
      "grad_norm": 41.7331428527832,
      "learning_rate": 1.1841326228537598e-05,
      "loss": 0.8502,
      "step": 1290
    },
    {
      "epoch": 2.3090586145648313,
      "grad_norm": 15.484479904174805,
      "learning_rate": 1.1545293072824156e-05,
      "loss": 0.8854,
      "step": 1300
    },
    {
      "epoch": 2.326820603907638,
      "grad_norm": 5.5273356437683105,
      "learning_rate": 1.1249259917110718e-05,
      "loss": 0.8407,
      "step": 1310
    },
    {
      "epoch": 2.3445825932504443,
      "grad_norm": 23.0351619720459,
      "learning_rate": 1.0953226761397278e-05,
      "loss": 0.8338,
      "step": 1320
    },
    {
      "epoch": 2.3623445825932503,
      "grad_norm": 23.307918548583984,
      "learning_rate": 1.0657193605683836e-05,
      "loss": 0.8638,
      "step": 1330
    },
    {
      "epoch": 2.380106571936057,
      "grad_norm": 4.491491794586182,
      "learning_rate": 1.0361160449970397e-05,
      "loss": 0.8788,
      "step": 1340
    },
    {
      "epoch": 2.3978685612788633,
      "grad_norm": 24.853282928466797,
      "learning_rate": 1.0065127294256957e-05,
      "loss": 0.77,
      "step": 1350
    },
    {
      "epoch": 2.4156305506216698,
      "grad_norm": 5.411221504211426,
      "learning_rate": 9.769094138543517e-06,
      "loss": 0.8925,
      "step": 1360
    },
    {
      "epoch": 2.4333925399644762,
      "grad_norm": 38.55131912231445,
      "learning_rate": 9.473060982830077e-06,
      "loss": 0.7079,
      "step": 1370
    },
    {
      "epoch": 2.4511545293072823,
      "grad_norm": 9.021842002868652,
      "learning_rate": 9.177027827116637e-06,
      "loss": 0.9026,
      "step": 1380
    },
    {
      "epoch": 2.4689165186500888,
      "grad_norm": 14.020875930786133,
      "learning_rate": 8.880994671403197e-06,
      "loss": 0.7558,
      "step": 1390
    },
    {
      "epoch": 2.4866785079928952,
      "grad_norm": 21.940189361572266,
      "learning_rate": 8.584961515689757e-06,
      "loss": 0.9276,
      "step": 1400
    },
    {
      "epoch": 2.5044404973357017,
      "grad_norm": 5.001364231109619,
      "learning_rate": 8.288928359976319e-06,
      "loss": 0.8212,
      "step": 1410
    },
    {
      "epoch": 2.522202486678508,
      "grad_norm": 6.191908359527588,
      "learning_rate": 7.992895204262878e-06,
      "loss": 0.778,
      "step": 1420
    },
    {
      "epoch": 2.5399644760213143,
      "grad_norm": 17.246042251586914,
      "learning_rate": 7.696862048549438e-06,
      "loss": 0.9104,
      "step": 1430
    },
    {
      "epoch": 2.5577264653641207,
      "grad_norm": 15.860824584960938,
      "learning_rate": 7.400828892835998e-06,
      "loss": 0.7855,
      "step": 1440
    },
    {
      "epoch": 2.575488454706927,
      "grad_norm": 6.613461017608643,
      "learning_rate": 7.1047957371225574e-06,
      "loss": 0.8641,
      "step": 1450
    },
    {
      "epoch": 2.5932504440497337,
      "grad_norm": 9.152128219604492,
      "learning_rate": 6.808762581409118e-06,
      "loss": 0.8609,
      "step": 1460
    },
    {
      "epoch": 2.61101243339254,
      "grad_norm": 123.00023651123047,
      "learning_rate": 6.512729425695678e-06,
      "loss": 0.7982,
      "step": 1470
    },
    {
      "epoch": 2.6287744227353462,
      "grad_norm": 13.364368438720703,
      "learning_rate": 6.216696269982238e-06,
      "loss": 0.9402,
      "step": 1480
    },
    {
      "epoch": 2.6465364120781527,
      "grad_norm": 30.55983543395996,
      "learning_rate": 5.920663114268799e-06,
      "loss": 0.9824,
      "step": 1490
    },
    {
      "epoch": 2.664298401420959,
      "grad_norm": 15.791711807250977,
      "learning_rate": 5.624629958555359e-06,
      "loss": 0.9217,
      "step": 1500
    },
    {
      "epoch": 2.6820603907637657,
      "grad_norm": 11.129791259765625,
      "learning_rate": 5.328596802841918e-06,
      "loss": 0.7721,
      "step": 1510
    },
    {
      "epoch": 2.699822380106572,
      "grad_norm": 10.393332481384277,
      "learning_rate": 5.032563647128479e-06,
      "loss": 0.782,
      "step": 1520
    },
    {
      "epoch": 2.717584369449378,
      "grad_norm": 16.158695220947266,
      "learning_rate": 4.736530491415039e-06,
      "loss": 0.8625,
      "step": 1530
    },
    {
      "epoch": 2.7353463587921847,
      "grad_norm": 19.235713958740234,
      "learning_rate": 4.4404973357015985e-06,
      "loss": 1.0254,
      "step": 1540
    },
    {
      "epoch": 2.753108348134991,
      "grad_norm": 21.736896514892578,
      "learning_rate": 4.144464179988159e-06,
      "loss": 0.8916,
      "step": 1550
    },
    {
      "epoch": 2.7708703374777977,
      "grad_norm": 33.014060974121094,
      "learning_rate": 3.848431024274719e-06,
      "loss": 0.7637,
      "step": 1560
    },
    {
      "epoch": 2.788632326820604,
      "grad_norm": 36.489803314208984,
      "learning_rate": 3.5523978685612787e-06,
      "loss": 0.7032,
      "step": 1570
    },
    {
      "epoch": 2.80639431616341,
      "grad_norm": 104.41853332519531,
      "learning_rate": 3.256364712847839e-06,
      "loss": 0.9263,
      "step": 1580
    },
    {
      "epoch": 2.8241563055062167,
      "grad_norm": 21.801990509033203,
      "learning_rate": 2.9603315571343994e-06,
      "loss": 1.0195,
      "step": 1590
    },
    {
      "epoch": 2.841918294849023,
      "grad_norm": 20.761587142944336,
      "learning_rate": 2.664298401420959e-06,
      "loss": 0.8352,
      "step": 1600
    },
    {
      "epoch": 2.8596802841918296,
      "grad_norm": 13.456282615661621,
      "learning_rate": 2.3682652457075193e-06,
      "loss": 0.9025,
      "step": 1610
    },
    {
      "epoch": 2.877442273534636,
      "grad_norm": 57.49467849731445,
      "learning_rate": 2.0722320899940796e-06,
      "loss": 0.7474,
      "step": 1620
    },
    {
      "epoch": 2.895204262877442,
      "grad_norm": 14.93920612335205,
      "learning_rate": 1.7761989342806394e-06,
      "loss": 0.9087,
      "step": 1630
    },
    {
      "epoch": 2.9129662522202486,
      "grad_norm": 55.252376556396484,
      "learning_rate": 1.4801657785671997e-06,
      "loss": 0.8793,
      "step": 1640
    },
    {
      "epoch": 2.930728241563055,
      "grad_norm": 19.79884147644043,
      "learning_rate": 1.1841326228537596e-06,
      "loss": 0.7856,
      "step": 1650
    },
    {
      "epoch": 2.9484902309058616,
      "grad_norm": 16.02100944519043,
      "learning_rate": 8.880994671403197e-07,
      "loss": 0.8115,
      "step": 1660
    },
    {
      "epoch": 2.966252220248668,
      "grad_norm": 10.219527244567871,
      "learning_rate": 5.920663114268798e-07,
      "loss": 0.6792,
      "step": 1670
    },
    {
      "epoch": 2.984014209591474,
      "grad_norm": 18.572072982788086,
      "learning_rate": 2.960331557134399e-07,
      "loss": 0.7667,
      "step": 1680
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.582,
      "eval_f1": 0.5894063851316189,
      "eval_loss": 0.8657308220863342,
      "eval_precision": 0.6100977589927719,
      "eval_recall": 0.582,
      "eval_runtime": 165.7805,
      "eval_samples_per_second": 3.016,
      "eval_steps_per_second": 0.38,
      "step": 1689
    }
  ],
  "logging_steps": 10,
  "max_steps": 1689,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3552031139328000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
